<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="How to use our Bulk HFile import feature to rapidly import large datasets into your Splice Machine database.">
<meta name="keywords" content=" bulk import, bulk load, bulk data load, hfile import, hfile load, s3, aws, importing from">
<title>Importing Data into Your Splice Machine Database | Splice Machine Documentation</title>
<meta class="swiftype" name="title" data-type="string" content="Importing Data into Your Splice Machine Database">
<meta class="swiftype" name="info" data-type="string" content="How to use our Bulk HFile import feature to rapidly import large datasets into your Splice Machine database.">

<link rel="stylesheet" href="css/syntax.css">


<link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
<link rel="stylesheet" href="css/modern-business.css">
<link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
<!--<link rel="stylesheet" href="css/lavish-bootstrap.css"> -->
<!--<link rel="stylesheet" href="css/customstyles.css"> -->
<link rel="stylesheet" href="css/splicemain.css">

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.4/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="js/jquery.navgoco.min.js"></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/2.0.0/anchor.min.js"></script>
<script src="js/toc.js"></script>
<script src="js/customscripts.js"></script>

<link rel="shortcut icon" href="images/favicon.ico">

<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link rel="alternate" type="application/rss+xml" title="" href="http://localhost:4000/feed.xml">

    <script>
        $(document).ready(function() {
            // Initialize navgoco with default options
            $("#mysidebar").navgoco({
                caretHtml: '',
                accordion: true,
                openClass: 'active', // open
                save: false, // leave false or nav highlighting doesn't work right
                cookie: {
                    name: 'navgoco',
                    expires: false,
                    path: '/'
                },
                slide: {
                    duration: 400,
                    easing: 'swing'
                }
            });

            $("#collapseAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', false);
            });

            $("#expandAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', true);
            });

        });

    </script>
    <script>
        $(function () {
            $('[data-toggle="tooltip"]').tooltip()
        })
    </script>
    <script>
        $(document).ready(function() {
            $("#tg-sb-link").click(function() {
                $("#tg-sb-sidebar").toggle();
                $("#tg-sb-content").toggleClass('col-md-9');
                $("#tg-sb-content").toggleClass('col-md-12');
                $("#tg-sb-icon").toggleClass('fa-toggle-on');
                $("#tg-sb-icon").toggleClass('fa-toggle-off');
            });
        });
    </script>
    

</head>
<body>
<!-- Navigation -->
<nav class="navbar navbar-inverse navbar-static-top">
    <div class="container-fluid topnavlinks">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="fa fa-home fa-lg navbar-brand" href="index.html">&nbsp;<span class="projectTitle"> Splice Machine Internal Docs</span></a>
        </div>
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <!-- toggle sidebar button -->
                <li><a id="tg-sb-link" href="#"><i id="tg-sb-icon" class="fa fa-toggle-on"></i> Nav</a></li>
                <!-- entries without drop-downs appear here -->
                
                    
                
                <!-- entries with drop-downs appear here -->
                <!-- conditional logic to control which topnav appears for the audience defined in the configuration file.-->
                
                   
                      <li class="dropdown">
                          <a href="#" class="dropdown-toggle" data-toggle="dropdown">Splice Machine<b class="caret"></b></a>
                          <ul class="dropdown-menu">
                             
                                
                                   <li><a href="index.html">Welcome</a></li>
                               
                             
                                
                                   <li><a href="tutorials_intro.html">Tutorials</a></li>
                               
                             
                                
                                   <li><a href="sqlref_intro.html">SQL Reference Manual</a></li>
                               
                             
                                
                                   <li><a href="developers_intro.html">Developer's Manual</a></li>
                               
                             
                                
                                   <li><a href="cmdlineref_intro.html">Command Line Reference</a></li>
                               
                             
                                
                                   <li><a href="dbconsole_intro.html">Database Console</a></li>
                               
                             
                                
                                   <li><a href="notes_intro.html">General Information</a></li>
                               
                             
                          </ul>
                      </li>
                   
                      <li class="dropdown">
                          <a href="#" class="dropdown-toggle" data-toggle="dropdown">DB-Service Only<b class="caret"></b></a>
                          <ul class="dropdown-menu">
                             
                                
                                   <li><a href="dbaas_intro.html">Welcome!</a></li>
                               
                             
                                
                                   <li><a href="dbaas_cm_intro.html">Cloud Manager Guide</a></li>
                               
                             
                                
                                   <li><a href="dbaas_zep_intro.html">Using Zeppelin</a></li>
                               
                             
                                
                                   <li><a href="dbaas_info_intro.html">Release Information</a></li>
                               
                             
                          </ul>
                      </li>
                   
                      <li class="dropdown">
                          <a href="#" class="dropdown-toggle" data-toggle="dropdown">On-Premise-DB Only<b class="caret"></b></a>
                          <ul class="dropdown-menu">
                             
                                
                                   <li><a href="onprem_intro.html">Welcome!</a></li>
                               
                             
                                
                                   <li><a href="onprem_install_intro.html">Installation</a></li>
                               
                             
                                
                                   <li><a href="onprem_admin_intro.html">Administrator's Guide</a></li>
                               
                             
                                
                                   <li><a href="onprem_info_intro.html">Release Information</a></li>
                               
                             
                          </ul>
                      </li>
                   
                
                <li>
<!-- Start Swiftype search -->
                    <div id="swiftype-search-container">
                        <form>
                          <input type="text" class="st-default-search-input swiftype-input">
                          <button type="submit" class="swiftype-submit">Search</button>
                        </form>
                    </div>
                </li>
            </ul>
        </div>
        </div>
        <!-- /.container -->
</nav>

<!-- Page Content -->
<div class="container-fluid">
    <!-- Content Row -->
    <div class="row">
        
        
            <!-- Sidebar Column -->
            <div class="col-md-3" id="tg-sb-sidebar">
                

<ul id="mysidebar" class="nav">
    <li class="sidebarTitle">Splice Machine 2.6.1</li>
    
    
    
    <li>
        <a href="#">Splice Machine Tutorials</a>
        <ul>
            
            
            
            <li><a href="tutorials_intro.html">Introduction</a></li>
            
            
            
            
        </ul>
     </li>
       
        
    
    <li>
        <a href="#">splice> Command Line</a>
        <ul>
            
            
            
            <li><a href="tutorials_cli_usingcli.html">Getting Started With splice></a></li>
            
            
            
            
            
            
            <li><a href="tutorials_cli_scripting.html">Scripting splice></a></li>
            
            
            
            
            
            
            <li><a href="tutorials_cli_rlwrap.html">RlWrap Summary</a></li>
            
            
            
            
        </ul>
     </li>
       
        
    
    <li>
        <a href="#">Ingestion and Streaming</a>
        <ul>
            
            
            
            <li><a href="tutorials_ingest_uploadtos3.html">Uploading Data to S3</a></li>
            
            
            
            
            
            
            <li><a href="tutorials_ingest_configures3.html">Setting up S3 Access</a></li>
            
            
            
            
            
            
            <li><a href="tutorials_ingest_tpch1.html">Importing TPCH Data</a></li>
            
            
            
            
            
            
            <li><a href="tutorials_ingest_importing.html">Importing Your Data</a></li>
            
            
            
            
            
            
            <li class="active"><a href="tutorials_ingest_hfiles.html">Importing Your Data as HFiles</a></li>
            
            
            
            
            
            
            <li><a href="tutorials_ingest_kafkaproducer.html">Creating a Kafka Producer</a></li>
            
            
            
            
            
            
            <li><a href="tutorials_ingest_kafkafeed.html">Configuring a Kafka Feed</a></li>
            
            
            
            
            
            
            <li><a href="tutorials_ingest_storm.html">Using Apache Storm</a></li>
            
            
            
            
            
            
            <li><a href="tutorials_ingest_mqttSpark.html">MQTT&#160;Spark Streaming</a></li>
            
            
            
            
        </ul>
     </li>
       
        
    
    <li>
        <a href="#">Analytics and Machine Learning</a>
        <ul>
            
            
            
            <li><a href="tutorials_ml_zeppelin.html">Using Zeppelin</a></li>
            
            
            
            
        </ul>
     </li>
       
        
    
    <li>
        <a href="#">Connecting Programmatically</a>
        <ul>
            
            
            
            <li><a href="tutorials_connect_haproxy.html">Connecting through HAProxy</a></li>
            
            
            
            
            
            
            <li><a href="tutorials_connect_java.html">Connecting with Java</a></li>
            
            
            
            
            
            
            <li><a href="tutorials_connect_jruby.html">Connecting with JRuby</a></li>
            
            
            
            
            
            
            <li><a href="tutorials_connect_jython.html">Connecting with Jython</a></li>
            
            
            
            
            
            
            <li><a href="tutorials_connect_scala.html">Connecting with Scala</a></li>
            
            
            
            
            
            
            <li><a href="tutorials_connect_angular.html">Connecting with NodeJS</a></li>
            
            
            
            
            
            
            <li><a href="tutorials_connect_odbcinstall.html">Installing our ODBC&#160;Driver</a></li>
            
            
            
            
            
            
            <li><a href="tutorials_connect_python.html">Connecting with Python</a></li>
            
            
            
            
            
            
            <li><a href="tutorials_connect_odbcc.html">Connecting with C&#160;and ODBC</a></li>
            
            
            
            
        </ul>
     </li>
       
        
    
    <li>
        <a href="#">Connecting BI Tools</a>
        <ul>
            
            
            
            <li><a href="tutorials_connect_dbeaver.html">Connecting DBeaver</a></li>
            
            
            
            
            
            
            <li><a href="tutorials_connect_dbvisualizer.html">Connecting DBVisualizer</a></li>
            
            
            
            
            
            
            <li><a href="tutorials_connect_cognos.html">Connecting Cognos</a></li>
            
            
            
            
            
            
            <li><a href="tutorials_connect_squirrel.html">Connecting SQuirreL</a></li>
            
            
            
            
            
            
            <li><a href="tutorials_connect_tableau.html">Connecting Tableau</a></li>
            
            
            
            
        </ul>
     </li>
       
        
        
        <!-- if you aren't using the accordion, uncomment this block:
           <p class="external">
               <a href="#" id="collapseAll">Collapse All</a> | <a href="#" id="expandAll">Expand All</a>
           </p>
           -->
</ul>

<!-- this highlights the active parent class in the navgoco sidebar. this is critical so that the parent expands when you're viewing a page. This must appear below the sidebar code above. Otherwise, if placed inside customscripts.js, the script runs before the sidebar code runs and the class never gets inserted.-->
<script>$("li.active").parents('li').toggleClass("active");</script>

            </div>
            
        

        <!-- Content Column -->
        <div class="col-md-9" id="tg-sb-content">
            

<div class="post-content">

<!--- Don't automatically put page summary at top - GRH 05/2017:
   
    <div class="summary">How to use our Bulk HFile import feature to rapidly import large datasets into your Splice Machine database.</div>
   
-->

    


    

  <section>
<div class="TopicContent" data-swiftype-index="true">
    <h1 id="importing-data-in-hfile-format-into-splice-machine">Importing Data in HFile Format Into Splice Machine</h1>

    <p>This tutorial describes how to import data using HFiles into your Splice
Machine database, and includes a number of examples. It also contains
specific tips to help you with the details of getting your data
correctly imported. This tutorial contains these sections:</p>

    <ul>
      <li><em>How Importing Your Data as HFiles Works</em> presents an overview of
using the HFile import functions.</li>
      <li><em>Usage Notes</em> provides an overview of the steps you use to import your
data as HFiles.</li>
      <li><em>Examples</em> contains two example of importing data in HFile format.</li>
      <li><a href="#Tips">Tips for Importing Data into Splice Machine</a> provides specific
tips for specifying your import parameters.</li>
    </ul>

    <p>If you’re importing data from an S3 bucket on AWS, please review our
<a href="tutorials_ingest_configures3.html">Configuring an S3 Bucket for Splice Machine
Access</a> tutorial before proceeding.</p>

    <p class="noteIcon">Our <a href="tutorials_ingest_importing.html">Importing Your Data tutorial</a>
walks you through using our standard import procedure, which is easier
to use, though slightly slower than importing HFiles.</p>

    <h2 id="How">How Importing Your Data as HFiles Works</h2>

    <p>Our HFile data import procedure leverages HBase bulk loading, which
allows it to import your data at a faster rate; however, using this
procedure instead of our standard
<a href="tutorials_ingest_importing.html"><code>SYSCS_UTIL.IMPORT_DATA</code></a> procedure
means that <em>constraint checks are not performing during data
importation</em>.</p>

    <p>You import a table as HFiles using our <code>SYSCS_UTIL.BULK_IMPORT_HFILES</code>
procedure, which temporarily converts the table file that you’re
importing into HFiles, imports those directly into your database, and
then removes the temporary HFiles. Before it generate HFiles,
<code>SYSCS_UTIL.BULK_IMPORT_HFILES</code> must determine how to split the data
into multiple regions by looking at the primary keys and figuring out
which values will yield relatively evenly-sized splits; the objective is
to compute splits such that roughly the same number of table rows will
end up in each split.</p>

    <p><code>SYSCS_UTIL.BULK_IMPORT_HFILES</code> can scan and analyze your table to
determine the best splits; or you can exercise control over those
splits.</p>

    <p>To have <code>SYSCS_UTIL.BULK_IMPORT_HFILES</code> calculate the splits
automatically, simply call this procedure with the <code>skipSampling</code>
parameter set to <code>true</code>.</p>

    <p>If you want to control the splits yourself, use these steps, which are
detailed in the <em>Examples</em> section below:</p>

    <ul>
      <li>You must determine which values make sense for splitting your data
into multiple regions. This means looking at the primary keys for the
table and figuring out which values will yield relatively evenly-sized
splits, which means that roughly the same number of table rows will
end up in each split.</li>
      <li>Calling one of our system procedures to compute the HBase-encoded keys
to create those splits.</li>
      <li>Calling one of our system procedures to set up the splits inside
your Splice Machine database.</li>
      <li>Call our <code>SYSCS_UTIL.BULK_IMPORT_HFILES</code> procedure, which temporarily
converts the table file that you’re importing into HFiles, imports
those directly into your database, and then removes the temporary
HFiles.</li>
    </ul>

    <p class="heading2"><a name="Usage"></a>Usage Notes</p>

    <div class="noteIcon">

      <p>Due to how Yarn manages memory, you need to modify your YARN configuration when bulk-importing large datasets. Make these two changes in your Yarn configuration:</p>

      <ul class="bulletCode">
        <li>
          <p>yarn.nodemanager.pmem-check-enabled=false</p>
        </li>
        <li>
          <p>yarn.nodemanager.vmem-check-enabled=false</p>
        </li>
      </ul>

    </div>

    <p class="body">As mentioned earlier, you can have <code>SYSCS_UTIL.BULK_IMPORT_HFILE</code>
automatically compute the splits for your table; it does so by sampling
the data in the table, generating a histogram, and then using that
histogram to compute the import splits. <code>SYSCS_UTIL.BULK_IMPORT_HFILE</code>
automatically computes the splits when the <code>skipSampling</code> parameter is
set to <code>false</code>.</p>

    <p class="heading3">Computing Split Keys for HFile Import</p>

    <p class="noteIcon">If you are using <code>skipSampling=false</code> to allow automatic computation of
splits, you can ignore the rest of this section, and skip to the
<a href="#Examples">Examples</a> section below.</p>

    <p>If you're computing splits for your import (and setting the
<code>skipSampling</code> parameter to <code>true</code>), you need to use these three
Splice Machine system procedures together:</p>

    <ul>
      <li><code>SYSCS_UTIL.COMPUTE_SPLIT_KEY</code> generates a keys file
*</li>
      <li>
        <p><code>SYSCS_UTIL.SYSCS_SPLIT_TABLE_OR_INDEX_AT_POINTS</code> sets up the splits
in Splice Machine</p>
      </li>
      <li><code>SYSCS_UTIL.BULK_IMPORT_HFILE</code> splits your input file into HFiles,
imports your data, and then removes the HFiles</li>
    </ul>

    <p>Alternatively, you can use
<a href="sqlref_sysprocs_splittable.html"><code>SYSCS_UTIL.SYSCS_SPLIT_TABLE_OR_INDEX</code></a>
with <code>SYSCS_UTIL.BULK_IMPORT_HFILE</code>. The
<code>SYSCS_UTIL.SYSCS_SPLIT_TABLE_OR_INDEX</code> system procedure combines the
functionality of <code>SYSCS_UTIL.COMPUTE_SPLIT_KEY</code> and
<code>SYSCS_UTIL.SYSCS_SPLIT_TABLE_OR_INDEX_AT_POINTS</code>.</p>

    <p>The process is as follows (and is shown in more detail in <em>Example 1</em>
and <em>Example 2</em> below):</p>

    <div class="opsStepsList">
      <ol class="boldFont">
        <li>
          <p class="topLevel">Create a directory on HDFS; for example:</p>

          <div class="preWrapperWide">
            <pre class="ShellCommand"><code>sudo -su hdfs hadoop fs -mkdir hdfs:///tmp/test_hfile_import
</code></pre>

          </div>

          <p class="indentLevel1">Make sure that the directory you create has permissions set to allow
Splice Machine to write your csv and Hfiles there.</p>
        </li>
        <li>
          <p class="topLevel">Determine primary key values that can horizontally split the table
into roughly equal sized partitions.</p>

          <div class="noteNote">
            <p>The value of your <code>hbase.hregion.max.filesize</code> setting; each
partition should ideally be about 1/2 of that value, so that the
region can grow after data is loaded.</p>

            <p>The sizes must be less than the <code>hbase.hregion.max.filesize value</code>.</p>

          </div>
        </li>
        <li>
          <p class="topLevel">Store those keys in a CSV file.</p>
        </li>
        <li>
          <p class="topLevel">Compute the split keys and then split the table.</p>

          <p class="indentLevel1">If you're using the combination of
 <a href="sqlref_sysprocs_computesplitkey.html"><code>SYSCS_UTIL.COMPUTE_SPLIT_KEY</code></a>
and
 <a href="sqlref_sysprocs_splittableatpoints.html"><code>SYSCS_UTIL.SYSCS_SPLIT_TABLE_OR_INDEX_AT_POINTS</code></a>:</p>

          <ol class="LowerAlphaPlainFont">
            <li>
              <p>Pass that CSV file into the
    <a href="sqlref_sysprocs_computesplitkey.html"><code>SYSCS_UTIL.COMPUTE_SPLIT_KEY</code></a> procedure
to compute the split row keys.</p>
            </li>
            <li>
              <p>Examine the computed (and encoded) row keys, which were written
to a file named keys in a subdirectory of your
<code>OutputDirectory</code>.</p>

              <p>The name of the subdirectory is the conglomerate ID for the
named table. You can find a table's conglomerate ID with the
 <a href="cmdlineref_showtables.html"><code>SHOW TABLES</code></a> command. </p>
            </li>
            <li>
              <p>Copy the encoded keys to your clipboard.</p>
            </li>
            <li>
              <p>Call the
 <a href="sqlref_sysprocs_splittableatpoints.html"><code>SYSCS_UTIL.SYSCS_SPLIT_TABLE_OR_INDEX_AT_POINTS</code></a> procedure,
passing in the row key values from your clipboard. This splits
your table inside the database.</p>
            </li>
          </ol>

          <p class="indentLevel1">If you are using the single procedure
 <a href="sqlref_sysprocs_splittable.html"><code>SYSCS_UTIL.SYSCS_SPLIT_TABLE_OR_INDEX</code></a>
instead:</p>

          <ol class="LowerAlphaPlainFont">
            <li>
              <p>Pass that CSV file into the  <a href="sqlref_sysprocs_splittable.html"><code>SYSCS_UTIL.SYSCS_SPLIT_TABLE_OR_INDEX</code></a> procedure.</p>
            </li>
            <li>
              <p>This computes the split row keys and split your table inside the database.</p>
            </li>
          </ol>
        </li>
        <li>
          <p class="topLevel">Repeat steps 1, 2, and 3 to split the indexes on your table.</p>
        </li>
        <li>
          <p class="topLevel">Call the
 <a href="sqlref_sysprocs_importhfile.html"><code>SYSCS_UTIL.BULK_IMPORT_HFILE</code></a> procedure
to split the input data file into HFiles and import the HFiles into
your Splice Machine database. The HFiles are deleted after being
imported.</p>
        </li>
      </ol>

    </div>

    <p class="heading2" id="Examples">Example 1</p>

    <p>This example details the steps used to import data in HFile format using
the Splice Machine <code>SYSCS_UTIL.BULK_IMPORT_HFILE</code> system procedure with
automatic splitting.</p>

    <p>Follow these steps :</p>

    <div class="opsStepsList">
      <ol class="boldFont">
        <li>
          <p class="topLevel">Create a directory on HDFS for the import; for example:</p>

          <div class="preWrapperWide">
            <pre class="ShellCommand"><code>sudo -su hdfs hadoop fs -mkdir hdfs:///tmp/test_hfile_import
</code></pre>

          </div>

          <p class="indentLevel1">Make sure that the directory you create has permissions set to allow
Splice Machine to write your csv and Hfiles there.</p>
        </li>
        <li>
          <p class="topLevel">Create table and index:</p>

          <div class="preWrapperWide">
            <pre class="Example"><code>CREATE TABLE TPCH.LINEITEM (
    L_ORDERKEY BIGINT NOT NULL,
    L_PARTKEY INTEGER NOT NULL,
    L_SUPPKEY INTEGER NOT NULL,
    L_LINENUMBER INTEGER NOT NULL,
    L_QUANTITY DECIMAL(15,2),
    L_EXTENDEDPRICE DECIMAL(15,2),
    L_DISCOUNT DECIMAL(15,2),
    L_TAX DECIMAL(15,2),
    L_RETURNFLAG VARCHAR(1),
    L_LINESTATUS VARCHAR(1),
    L_SHIPDATE DATE,
    L_COMMITDATE DATE,
    L_RECEIPTDATE DATE,
    L_SHIPINSTRUCT VARCHAR(25),
    L_SHIPMODE VARCHAR(10),
    L_COMMENT VARCHAR(44),
    PRIMARY KEY(L_ORDERKEY,L_LINENUMBER)
);

CREATE INDEX L_SHIPDATE_IDX on TPCH.LINEITEM(
    L_SHIPDATE,
    L_PARTKEY,
    L_EXTENDEDPRICE,
    L_DISCOUNT
);
</code></pre>

          </div>
        </li>
        <li>
          <p class="topLevel">Import the HFiles Into Your Database</p>

          <p class="indentLevel1">Once you have split your table and indexes, call this procedure to
generate and import the HFiles into your Splice Machine database:</p>

          <div class="preWrapperWide">
            <pre class="Example"><code>call SYSCS_UTIL.BULK_IMPORT_HFILE('TPCH', 'LINEITEM', null,
        '/TPCH/1/lineitem', '|', null, null, null, null, -1,
        '/BAD', true, null, 'hdfs:///tmp/test_hfile_import/', false);
</code></pre>

          </div>

          <p class="indentLevel1">The generated HFiles are automatically deleted after being imported.</p>
        </li>
      </ol>

    </div>

    <p class="heading2" id="Examples">Example 2</p>

    <p>The example in this section details the steps used to
import data in HFile format using the Splice Machine <code>SYSCS_UTIL.COMPUTE_SPLIT_KEY</code>,
<code>SYSCS_UTIL.SYSCS_SPLIT_TABLE_OR_INDEX_AT_POINTS</code>, and
<code>SYSCS_UTIL.BULK_IMPORT_HFILE</code> system procedures.</p>

    <p>Follow these steps :</p>

    <div class="opsStepsList">
      <ol class="boldFont">
        <li>
          <p class="topLevel">Create a directory on HDFS for the import; for example:</p>

          <div class="preWrapperWide">
            <pre class="ShellCommand"><code>sudo -su hdfs hadoop fs -mkdir hdfs:///tmp/test_hfile_import
</code></pre>

          </div>

          <p>Make sure that the directory you create has permissions set to allow
Splice Machine to write your csv and Hfiles there.</p>
        </li>
        <li>
          <p class="topLevel">Create table and index:</p>

          <div class="preWrapperWide">
            <pre class="Example"><code>CREATE TABLE TPCH.LINEITEM (
     L_ORDERKEY BIGINT NOT NULL,
     L_PARTKEY INTEGER NOT NULL,
     L_SUPPKEY INTEGER NOT NULL,
     L_LINENUMBER INTEGER NOT NULL,
     L_QUANTITY DECIMAL(15,2),
     L_EXTENDEDPRICE DECIMAL(15,2),
     L_DISCOUNT DECIMAL(15,2),
     L_TAX DECIMAL(15,2),
     L_RETURNFLAG VARCHAR(1),
     L_LINESTATUS VARCHAR(1),
     L_SHIPDATE DATE,
     L_COMMITDATE DATE,
     L_RECEIPTDATE DATE,
     L_SHIPINSTRUCT VARCHAR(25),
     L_SHIPMODE VARCHAR(10),
     L_COMMENT VARCHAR(44),
     PRIMARY KEY(L_ORDERKEY,L_LINENUMBER)
);
</code></pre>

          </div>
        </li>
        <li>
          <p class="topLevel">Compute the split row keys for the table:</p>

          <ol class="LowerAlphaPlainFont">
            <li>
              <p>Find primary key values that can horizontally split the table
into roughly equal sized partitions.</p>

              <p>For this example, we provide 3 keys in a file named
<code>lineitemKey.csv</code>. Note that each of our three keys includes a
second column that is <code>null</code>:</p>

              <div class="preWrapperWide">
                <pre class="Example"><code>1500000|3000000|4500000|
</code></pre>

              </div>
            </li>
            <li>
              <p>Specify the column names in the csv file in the <code>columnList</code>
parameter; in our example, the primary key columns are:</p>

              <div class="preWrapperWide">
                <pre class="Example"><code>'L_ORDERKEY,L_LINENUMBER'
</code></pre>

              </div>
            </li>
            <li>
              <p>Invoke <code>SYSCS_UTIL.COMPUTE_SPLIT_KEY</code> to compute hbase split row
keys and write them to a file:</p>

              <pre class="Example"><code>call SYSCS_UTIL.COMPUTE_SPLIT_KEY('TPCH', 'LINEITEM',
        null, 'L_ORDERKEY,L_LINENUMBER',
        'hdfs:///tmp/test_hfile_import/lineitemKey.csv',
        '|', null, null, null,
        null, -1, '/BAD', true, null, 'hdfs:///tmp/test_hfile_import/');
</code></pre>
            </li>
          </ol>
        </li>
        <li>
          <p class="topLevel">Set up the table splits in your database:</p>

          <ol class="LowerAlphaPlainFont">
            <li>
              <p>Use <code>SHOW TABLES</code> to discover the conglomerate ID for the
<code>TPCH.LINEITEM</code> table, which for this example is <code>1536</code>. This
means that the split keys file for this table is in the
<code>hdfs:///tmp/test_hfile_import/1536</code> directory. You'll see
values like these:</p>

              <div class="preWrapperWide">
                <pre class="Example"><code>\xE4\x16\xE3`\xE4-\xC6\xC0\xE4D\xAA
</code></pre>

              </div>
            </li>
            <li>
              <p>Now use those values in a call to our system procedure to split
the table inside the database:</p>

              <pre class="Example"><code>call SYSCS_UTIL.SYSCS_SPLIT_TABLE_OR_INDEX_AT_POINTS('TPCH','LINEITEM',
        null,'\xE4\x16\xE3`,\xE4-\xC6\xC0,\xE4D\xAA');
</code></pre>
            </li>
          </ol>
        </li>
        <li>
          <p class="topLevel">Compute the split keys for your index:</p>

          <ol class="LowerAlphaPlainFont">
            <li>
              <p>Find index values that can horizontally split the table into
roughly equal sized partitions.</p>
            </li>
            <li>
              <p>For this example, we provide 2 index values in a file named
<code>shipDateIndex.csv</code>. Note that each of our keys includes <code>null</code>
column values:</p>

              <div class="preWrapperWide">
                <pre class="Example"><code>1994-01-01|||
1996-01-01|||
</code></pre>

              </div>
            </li>
            <li>
              <p>Specify the column names in the csv file in the <code>columnList</code>
parameter; in our example, the index columns are:</p>

              <div class="preWrapperWide">
                <pre class="Example"><code>'L_SHIPDATE,L_PARTKEY,L_EXTENDEDPRICE,L_DISCOUNT'
</code></pre>

              </div>
            </li>
            <li>
              <p>Invoke <code>SYSCS_UTIL.COMPUTE_SPLIT_KEY</code> to compute hbase split row
keys and write them to a file:</p>

              <div class="preWrapperWide">
                <pre class="Example"><code>call SYSCS_UTIL.COMPUTE_SPLIT_KEY('TPCH', 'LINEITEM', 'L_SHIPDATE_IDX',
        'L_SHIPDATE,L_PARTKEY,L_EXTENDEDPRICE,L_DISCOUNT',
        'hdfs:///tmp/test_hfile_import/shipDateIndex.csv',
        '|', null, null, null, null, -1, '/BAD', true, null,
         'hdfs:///tmp/test_hfile_import/');
</code></pre>

              </div>
            </li>
          </ol>
        </li>
        <li>
          <p class="topLevel">Set up the indexes in your database:</p>

          <ol class="LowerAlphaPlainFont">
            <li>
              <p>Copy the row key values from the output file:</p>

              <div class="preWrapperWide">
                <pre class="Example"><code>\xEC\xB0Y9\xBC\x00\x00\x00\x00\x00\x80
\xEC\xBF\x08\x9C\x14\x00\x00\x00\x00\x00\x80
</code></pre>

              </div>
            </li>
            <li>
              <p>Now call our system procedure to split the index:</p>

              <div class="preWrapperWide">
                <pre class="Example"><code>call SYSCS_UTIL.SYSCS_SPLIT_TABLE_OR_INDEX_AT_POINTS(
        'TPCH','LINEITEM','L_SHIPDATE_IDX',
        '\xEC\xB0Y9\xBC\x00\x00\x00\x00\x00\x80,
        \xEC\xBF\x08\x9C\x14\x00\x00\x00\x00\x00\x80');
</code></pre>

              </div>
            </li>
          </ol>
        </li>
        <li>
          <p class="topLevel">Import the HFiles Into Your Database</p>

          <p class="indentLevel1">Once you have split your table and indexes, call this procedure to
generate and import the HFiles into your Splice Machine database:</p>

          <div class="preWrapperWide">
            <pre class="Example"><code>call SYSCS_UTIL.BULK_IMPORT_HFILE('TPCH', 'LINEITEM', null,
        '/TPCH/1/lineitem', '|', null, null, null, null, -1,
        '/BAD', true, null,
        'hdfs:///tmp/test_hfile_import/', true);
</code></pre>

          </div>

          <p class="indentLevel1">The generated HFiles are automatically deleted after being imported.</p>
        </li>
      </ol>

    </div>

    <p class="heading2">Example 3</p>

    <p>The example in this section details the steps used to import data in
HFile format using the Splice Machine
<code>SYSCS_UTIL.SYSCS_SPLIT_TABLE_OR_INDEX</code>, and
<code>SYSCS_UTIL.BULK_IMPORT_HFILE</code> system procedures.</p>

    <p>Follow these steps :</p>

    <div class="opsStepsList">
      <ol class="boldFont">
        <li>
          <p class="topLevel">Create a directory on HDFS for the import; for example:</p>

          <div class="preWrapperWide">
            <pre class="ShellCommand"><code>sudo -su hdfs hadoop fs -mkdir hdfs:///tmp/test_hfile_import
</code></pre>

          </div>

          <p>Make sure that the directory you create has permissions set to allow
Splice Machine to write your csv and Hfiles there.</p>
        </li>
        <li>
          <p class="topLevel">Create table and index:</p>

          <div class="preWrapperWide">
            <pre class="Example"><code>CREATE TABLE TPCH.LINEITEM (
    L_ORDERKEY BIGINT NOT NULL,
    L_PARTKEY INTEGER NOT NULL,
    L_SUPPKEY INTEGER NOT NULL,
    L_LINENUMBER INTEGER NOT NULL,
    L_QUANTITY DECIMAL(15,2),
    L_EXTENDEDPRICE DECIMAL(15,2),
    L_DISCOUNT DECIMAL(15,2),
    L_TAX DECIMAL(15,2),
    L_RETURNFLAG VARCHAR(1),
    L_LINESTATUS VARCHAR(1),
    L_SHIPDATE DATE,
    L_COMMITDATE DATE,
    L_RECEIPTDATE DATE,
    L_SHIPINSTRUCT VARCHAR(25),
    L_SHIPMODE VARCHAR(10),
    L_COMMENT VARCHAR(44),
    PRIMARY KEY(L_ORDERKEY,L_LINENUMBER)
    );

CREATE INDEX L_SHIPDATE_IDX on TPCH.LINEITEM(
    L_SHIPDATE,
    L_PARTKEY,
    L_EXTENDEDPRICE,
    L_DISCOUNT
    );
</code></pre>

          </div>
        </li>
        <li>
          <p class="topLevel">Compute the split row keys for your table and set up the split in
your database:</p>

          <ol class="LowerAlphaPlainFont">
            <li>
              <p>Find primary key values that can horizontally split the table
into roughly equal sized partitions.</p>

              <p>For this example, we provide 3 keys in a file named
<code>lineitemKey.csv</code>. Note that each of our three keys includes a
second column that is <code>null</code>:</p>

              <div class="preWrapperWide">
                <pre class="Example"><code>1500000|3000000|4500000|
</code></pre>

              </div>
            </li>
            <li>
              <p>Specify the column names in the csv file in the <code>columnList</code>
parameter; in our example, the primary key columns are:</p>

              <div class="preWrapperWide">
                <pre class="Example"><code>'L_ORDERKEY,L_LINENUMBER'
</code></pre>

              </div>
            </li>
            <li>
              <p>Invoke <code>SYSCS_UTIL.SYSCS_SPLIT_TABLE_OR_INDEX</code> to compute hbase
split row keys and set up the splits</p>

              <pre class="Example"><code>call SYSCS_UTIL.SYSCS_SPLIT_TABLE_OR_INDEX('TPCH',
        'LINEITEM',null, 'L_ORDERKEY,L_LINENUMBER',
        'hdfs:///tmp/test_hfile_import/lineitemKey.csv',
        '|', null, null, null,
        null, -1, '/BAD', true, null);
</code></pre>
            </li>
          </ol>
        </li>
        <li>
          <p class="topLevel">Compute the split keys for your index:</p>

          <ol class="LowerAlphaPlainFont">
            <li>
              <p>Find index values that can horizontally split the table into
roughly equal sized partitions.</p>
            </li>
            <li>
              <p>For this example, we provide 2 index values in a file named
<code>shipDateIndex.csv</code>. Note that each of our keys includes <code>null</code>
column values:</p>

              <div class="preWrapperWide">
                <pre class="Example"><code>1994-01-01|||
1996-01-01|||
</code></pre>

              </div>
            </li>
            <li>
              <p>Specify the column names in the csv file in the <code>columnList</code>
parameter; in our example, the index columns are:</p>

              <div class="preWrapperWide">
                <pre class="Example"><code>'L_SHIPDATE,L_PARTKEY,L_EXTENDEDPRICE,L_DISCOUNT'
</code></pre>

              </div>
            </li>
            <li>
              <p>Invoke <code>SYSCS_UTIL.SYSCS_SPLIT_TABLE_OR_INDEX</code> to compute hbase
split row keys and set up the index splits</p>

              <pre class="Example"><code>call SYSCS_UTIL.SYSCS_SPLIT_TABLE_OR_INDEX('TPCH',
        'LINEITEM', 'L_SHIPDATE_IDX',
        'L_SHIPDATE,L_PARTKEY,L_EXTENDEDPRICE,L_DISCOUNT',
        'hdfs:///tmp/test_hfile_import/shipDateIndex.csv',
        '|', null, null,
        null, null, -1, '/BAD', true, null);
</code></pre>
            </li>
          </ol>
        </li>
        <li>
          <p class="topLevel">Import the HFiles Into Your Database</p>

          <p class="indentLevel1">Once you have split your table and indexes, call this procedure to
generate and import the HFiles into your Splice Machine database:</p>

          <div class="preWrapperWide">
            <pre class="Example"><code>call SYSCS_UTIL.BULK_IMPORT_HFILE('TPCH', 'LINEITEM', null,
            '/TPCH/1/lineitem', '|', null, null, null, null,
            -1, '/BAD', true, null,
            'hdfs:///tmp/test_hfile_import/', true);
</code></pre>

          </div>

          <p class="indentLevel1">The generated HFiles are automatically deleted after being imported.</p>
        </li>
      </ol>

    </div>

    <h2 id="Tips">Tips for Importing Data into Splice Machine</h2>

    <p>This tutorial contains a number of tips that our users have found very
useful in determining the parameter settings to use when running an
import:</p>

    <ol>
      <li><a href="#Tip4">Tip #1:  Use Special Characters for Delimiters</a></li>
      <li><a href="#Tip5">Tip #2:  Avoid Problems With Date, Time, and Timestamp Formats</a></li>
      <li><a href="#Tip6">Tip #3:  Change the Bad Directory for Each Table Group</a></li>
      <li><a href="#Tip7">Tip #4:  Importing Multi-line Records</a></li>
      <li><a href="#Tip8">Tip #5:  Importing CLOBs and BLOBs</a></li>
      <li><a href="#Tip9">Tip #6:  Scripting Your Imports</a></li>
    </ol>

    <h3 id="Tip4">Tip #1:  Use Special Characters for Delimiters</h3>

    <p>One common gotcha we see with customer imports is when the data you’re
importing includes a special character that you’ve designated as a
column or character delimiter. You’ll end up with records in your bad
record directory and can spend hours trying to determine the issue, only
to discover that it’s because the data includes a delimiter character.
This can happen with columns that contain data such as product
descriptions.</p>

    <h4 id="column-delimiters">Column Delimiters</h4>

    <p>The standard column delimiter is a comma (<code>,</code>); however, we’ve all
worked with string data that contains commas, and have figured out to
use a different column delimiter. Some customers use the pipe
(<code>|</code>) character, but frequently discover that it is also used in some
descriptive data in the table they’re importing.</p>

    <p>We recommend using a control character like <code>CTRL-A</code> for your column
delimiter. This is known as the SOH character, and is represented by
0x01 in hexadecimal. Unfortunately, there’s no way to enter this
character from the keyboard in the Splice Machine command line
interface; instead, you need to create a script file (see <a href="#Tip9">Tip
#9</a>) and type the control character using a text editor like <em>vi</em>
or <em>vim</em>:</p>

    <ul>
      <li>Open your script file in vi or vim.</li>
      <li>Enter into INSERT mode.</li>
      <li>Type <code>CTRL-V</code> then <code>CTRL-A</code> for the value of the column delimiter
parameter in your procedure call. Note that this typically echoes as
<code>^A</code> when you type it in vi or vim.</li>
    </ul>

    <h4 id="character-delimiters">Character Delimiters</h4>

    <p>By default, the character delimiter is a double quote. This can produce
the same kind of problems that we see with using a comma for the column
delimiter: columns values that include embedded quotes or use the double
quote as the symbol for inches. You can use escape characters to include
the embedded quotes, but it’s easier to use a special character for your
delimiter.</p>

    <p>We recommend using <code>CTRL-G</code>, which you can add to a script file (see
<a href="#Tip9">Tip #9</a>), again using a text editor like <em>vi</em> or <em>vim</em>:</p>

    <ul>
      <li>Open your script file in vi or vim.</li>
      <li>Enter into INSERT mode.</li>
      <li>Type <code>CTRL-V</code> then <code>CTRL-G</code> for the value of the character delimiter
parameter in your procedure call. Note that this typically echoes as
<code>^G</code> when you type it in vi or vim.</li>
    </ul>

    <h3 id="Tip5">Tip #2:  Avoid Problems With Date, Time, and Timestamp Formats</h3>

    <p>Perhaps the most common difficulty that customers have with importing
their data is with date, time, and timestamp values.</p>

    <p>Splice Machine adheres to the Java <code>SimpleDateFormat</code> syntax for all
date, time, and timestamp values, <code>SimpleDateFormat</code> is described here:</p>

    <p class="indentLevel1"><a href="https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html" target="_blank">https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html</a></p>

    <p>Splice Machine’s implementation of <code>SimpleDateFormat</code> is case-sensitive;
this means, for example, that a lowercase <code>h</code> is used to represent an
hour value between 0 and 12, whereas an uppercase <code>H</code> is used to
represent an hour between 0 and 23.</p>

    <p>Splice Machine’s Import procedures only allow you to specify one format
each for the date, time, and timestamp columns in the table data you are
importing. This means that, for example, every date in the table data
must be in the same format.</p>

    <div class="notePlain">
      <p>All of the <code>Date</code> values in the file (or group of files) you are
importing must use the same date format.</p>

      <p>All of the <code>Time</code> values in the file (or group of files) you are
importing must use the same time format.</p>

      <p>All of the <code>Timestamp</code> values in the file (or group of files) you are
importing must use the same timestamp format.</p>

    </div>
    <p>A few additional notes:</p>

    <ul>
      <li>The <code>Timestamp</code> data type has a range of <code>1678-01-01</code> to <code>2261-12-31</code>.
Some customers have used dummy timestamp values like <code>9999-01-01</code>,
which will fail because the value is out of range for a timestamp.
Note that this is not an issue with <code>Date</code> values.</li>
      <li>Splice Machine suggests that, if your data contains any date or
timestamp values that are not in the format <code>yyyy-MM-dd HH:mm:ss</code>, you
create a simple table that has just one or two columns and test
importing the format. This is a simple way to confirm that the
imported data is what you expect.</li>
    </ul>

    <h3 id="Tip6">Tip #3:  Change the Bad Directory for Each Table / Group</h3>

    <p>If you are importing a large amount of data and have divided the files
you are importing into groups, then it’s a good idea to change the
location of the bad record directory for each group; this will make
debugging bad records a lot easier for you.</p>

    <p>You can change the value of the <code>badRecordDirectory</code> to include your
group name; for example, we typically use a strategy like the following:</p>

    <table style="width: 100%;">
                <col />
                <col />
                <thead>
                    <tr>
                        <th>Group Files Location</th>
                        <th><span class="CodeBoldFont">badRecordDirectory</span> Parameter Value</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><code>/data/mytable1/group1</code></td>
                        <td><code>/BAD/mytable1/group1</code></td>
                    </tr>
                    <tr>
                        <td><code>/data/mytable1/group2</code></td>
                        <td><code>/BAD/mytable1/group2</code></td>
                    </tr>
                    <tr>
                        <td><code>/data/mytable1/group3</code></td>
                        <td><code>/BAD/mytable1/group3</code></td>
                    </tr>
                </tbody>
            </table>
    <p>You’ll then be able to more easily discover where the problem record is
located.</p>

    <h3 id="Tip7">Tip #4:  Importing Multi-line Records</h3>

    <p>If your data contains line feed characters like <code>CTRL-M</code>, you need to
set the <code>oneLineRecords</code> parameter to <code>false</code>. Splice Machine will
accommodate to the line feeds; however, the import will take longer
because Splice Machine will not be able to break the file up and
distribute it across the cluster.</p>

    <p class="notePlain">To improve import performance, avoid including line feed characters in
your data and set the <code>oneLineRecords</code> parameter to <code>true</code>.</p>

    <h3 id="Tip8">Tip #5:  Importing CLOBs and BLOBs</h3>

    <p>If you are importing <code>CLOB</code>s, pay careful attention to tips <a href="#Tip4">4</a>
and <a href="#Tip7">7</a>. Be sure to use special characters for both your column
and character delimiters. If your <code>CLOB</code> data can span multiple lines,
be sure to set the <code>oneLineRecords</code> parameter to <code>false</code>.</p>

    <p>At this time, the Splice Machine import procedures do not import work
with columns of type <code>BLOB</code>. You can create a virtual table interface
(VTI) that reads the <code>BLOB</code>s and inserts them into your database.</p>

    <h3 id="Tip9">Tip #6:  Scripting Your Imports</h3>

    <p>You can make import tasks much easier and convenient by creating <em>import
scripts</em>. An import script is simply a call to one of the import
procedures; once you’ve verified that it works, you can use and clone
the script and run unattended imports.</p>

    <p>An import script is simply a file in which you store <code>splice&gt;</code> commands
that you can execute with the <code>run</code> command. For example, here’s an
example of a text file named <code>myimports.sql</code> that we can use to import
two csv files into our database:</p>

    <div class="preWrapperWide">
      <pre class="Example"><code>call SYSCS_UTIL.IMPORT_DATA ('SPLICE','mytable1',null,'/data/mytable1/data.csv',null,null,null,null,null,0,'/BAD/mytable1',null,null);call SYSCS_UTIL.IMPORT_DATA ('SPLICE','mytable2',null,'/data/mytable2/data.csv',null,null,null,null,null,0,'/BAD/mytable2',null,null
</code></pre>

    </div>
    <p>To run an import script, use the <code>splice&gt; run</code> command; for example:</p>

    <div class="preWrapper">
      <pre class="Example"><code>splice&gt; run 'myimports.sql';
</code></pre>

    </div>
    <p>You can also start up the <code>splice&gt;</code> command line interpreter with the
name of a file to run; for example:</p>

    <div class="preWrapper">
      <pre class="Example"><code>sqlshell.sh -f myimports.sql
</code></pre>

    </div>
    <p>In fact, you can script almost any sequence of Splice Machine commands
in a file and run that script within the command line interpreter or
when you start the interpreter.</p>

  </div>
</section>



    <div class="tags">
        
    </div>



</div>

<hr class="shaded"/>

<footer>
            <div class="row">
                <div class="col-lg-12 footer">
               &copy;2017 Splice Machine, Inc. All rights reserved. <br />
 Site last generated: Nov 2, 2017 <br />
<p><img src="images/company_logo.png" alt="Company logo"/></p>
                </div>
            </div>
</footer>


        </div>
    <!-- /.row -->
</div>
<!-- /.container -->
    </div>

        <!-- Swiftype script for docstest -->

    <script type="text/javascript">
    (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
    })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');
      _st('install','YGtH3zGt4NJzs35yCo5A','2.0.0');
    </script>




</body>
    <!-- Only include analytics scripts if generating the customer docs -->


</html>
