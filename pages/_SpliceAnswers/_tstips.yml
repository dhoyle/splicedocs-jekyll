entries:
- title: kbinfo
  categories:
    - category: Cluster Configuration
      internal_only: No
      description: This section contains __troubleshooting__ help for Splice Machine cluster configuration issues.
      issues:

      - title: Node Manager Sizing
        internal_only: No
        description: Set size of NodeManager to `4GB`.
        conditions: For larger clusters
        discussion: >
          We did this on 100 Node Anthem cluster with parallel loads when the default value (1GB) could cause TPCH queries to fail with this error message:\n

          ```
          ShuffleMapStage 38547 (Scalar Aggregate) has failed the maximum allowable number of times: 4
          ```
          {: .Example}\n
        comments: >
            See also: [https://stackoverflow.com/a/34942339/1001523]

      - title: Set HFile Block Cache Size
        internal_only: No
        description: >
          Decrease `hfile.block.cache.size` in `hbase-site.xml` for client to `0.001` (instead of `0.1`)
        conditions: >
          __Needs discussion:__ There seems to be some argument on the proper settings and under the proper conditions (0.4? 0.1? 0.01? 0.001?)
        discussion: >
          In HBase, the property `hfile.block.cache.size` defaults to `0.4`, but that is counterproductive to efficient use of our spark executors.
        comments:

    - category: Customer-Specific
      internal_only: Yes
      description: This section contains configuration information for specific Splice Machine customers.
      issues:

      - title: Anthem Settings
        internal_only: Yes
        description: Settings, recommendations, and reasons for Anthem configuration.
        conditions:
        discussion: >
          See: [https://docs.google.com/spreadsheets/d/1H757Cc79IuWQ7OZyvwDnuu4d4MSxJIlQQS7mYKbhLTM/edit?usp=sharing]
        comments:

    - category: Database Configuration
      internal_only: No
      description: This section contains information related to configuring the Splice Machine database in a cluster.
      issues:

      - title: Configure Write Pipeline
        internal_only: No
        description: Correct configuration of write threads counts
        conditions: Always
        discussion: >
            Increase these values to 40% of the handler count:

            * `splice.independent.write.threads`
            *  `splice.dependent.write.threads`
        comments: __Needs Discussion__

      - title: Enable Spark2 Shuffle
        internal_only: No
        description: Make sure that Spark2 shuffle is enabled and used
        conditions: Always
        discussion:
        comments:

      - title: Reduce Parser Overhead
        internal_only: No
        description: "You can reduce parser overhead for multiple `IN` lists when there are a low number of rows per combined column probe key."
        conditions: Depending on data demographics.
        discussion: "Use the `SET_GLOBAL_DATABASE_PROPERTY` system procedure; for example:\n\n`CALL SYSCS_UTIL.SYSCS_SET_GLOBAL_DATABASE_PROPERTY('derby.database.maxMulticolumnProbeValues', '2000');`\n\nThe default limit is `10000` probes."
        comments: "MultiProbeScan with multiple `IN` lists can be tuned, depending on data demographics, to limit the maximum number of probes that are generated. This avoids unnecessary parser overhead when there are a low number of rows per combined column probe key."

    - category: Database Maintenance
      internal_only: No
      description: This section contains information related to maintaining your Splice Machine database.
      issues:

      - title: Flushing memstore
        internal_only: No
        description: "How to flush memstore for a specific table to HDFS without using the HBase shell."
        conditions: As needed
        discussion: "`CALL SYSCS_UTIL.SYSCS_FLUSH_TABLE('Schema_Name','Table_Name');`"
        comments:

      - title: Restoring Missing Disk Space
        internal_only: No
        description: Disk space for a deleted database object is not recovered until the process completes.
        conditions: When disk space is running low
        discussion:
        comments: "Other tricks in this ticket include\n* running commands to find still-running jobs still holding on to (deleted) files\n * commands to restart cloudera agent from command line"

    - category: Debugging
      internal_only: No
      description: This section contains information about debugging database issues.
      issues:

      - title: Fix System Table Corruption
        internal_only: No
        description: Use the `DICTIONARY_DELETE` system procedure to fix corruption.
        conditions: When indexes have more or less rows than the base table.
        discussion: "Write queries to find the affected `ROWIDs`, using the `index=<index_name>` hint to specify the source, which can be an index or the base table."
        comments:

      - title: Submitting Exception Call Stacks
        internal_only: Yes
        description: Always grab the last listed Exception call stack from the logs.
        conditions: "A customer reports a system error, and there is a chain of Exception stack traces in the applicable log file, e.g.:\n\n```SQLException ... caused by StandardException... caused by IOException```\n{: .Example}"
        discussion: "Always save the last-listed call stack in the set, as it indicates the original root cause, and is essential for problem analysis.\n\nSaving the entire set of stack traces may be best.\n\nIf the call stack indicates a remote exception, the original stack trace from a different node in the cluster might not be listed, and may have to be searched for in logs on the other nodes."
        comments:


    - category: External Tables
      internal_only: No
      description: This section contains information about working with external tables.
      issues:

      - title: Ingestion of bad records
        internal_only: No
        description: Bad import record log files may be difficult to access in a fully secured Kerberos environment.
        conditions: When ingesting data from an external table in a secured Kerberos environment, and the data contains bad records.
        discussion: "Write queries to find the affected `ROWIDs`, using the `index=<index_name>` hint to specify the source, which can be an index or the base table."
        comments:
