---
title: Splice Machine Native Spark DataSource Methods
summary: Methods Available in the Splice Machine Native Spark DataSource.
keywords: spark, adapter, splicemachineContext
toc: false
compatible_version: 2.7
product: all
sidebar: developers_sidebar
permalink: developers_spark_methods.html
folder: DeveloperTopics/Spark
---
<section>
<div class="TopicContent" data-swiftype-index="true" markdown="1">

# The Native Spark DataSource Methods {#methods}

This topic describes the following methods of the `SplicemachineContext` class:

* [`analyzeSchema`](#analyzeschema)
* [`analyzeTable`](#analyzetable)
* [`bulkImportHFile`](#bulkimporthfile)
* [`createTable`](#createtable)
* [`delete`](#delete)
* [`df` and `internalDf`](#df)
* [`dropTable`](#droptable)
* [`export`](#export)
* [`exportBinary`](#exportbinary)
* [`getConnection`](#getconnection)
* [`getSchema`](#getschema)
* [`insert`](#insert)
* [`rdd` and `internalRdd`](#rdd)
* [`splitAndInsert`](#splitandinsert)
* [`tableExists`](#tableexists)
* [`truncateTable`](#truncatetable)
* [`update`](#update)
* [`upsert`](#upsert)

## analyzeSchema {#analyzeSchema}

This method collects statistics for an entire schema; it is the same as using the [`ANALYZE SCHEMA`](cmdlineref_analyze.html) `splice>` command line.

<div class="fcnWrapperWide" markdown="1"><pre>
analyzeSchema(schemaName: String): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
schemaName
{: .paramName}

The name of the schema that you want analyzed.
{: .paramDefnFirst}
</div>


## analyzeTable

This method collects statistics for a specific table; it is the same as using the [`ANALYZE TABLE`](cmdlineref_analyze.html) `splice>` command line.

<div class="fcnWrapperWide" markdown="1"><pre>
analyzeTable( tableName: String,
              estimateStatistics: Boolean = false,
              samplePercent: Double = 0.10 ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
tableName
{: .paramName}

The name of the table that you want analyzed.
{: .paramDefnFirst}

estimateStatistics
{: .paramName}

A Boolean that specifies whether you want statistics generated by sampling the specified sampling percentage of the table. This can significantly reduce the overhead associated with generating statistics. Setting this parameter to `false` specifies that statistics are to be generated based on the full table.
{: .paramDefnFirst}

samplePercent
{: .paramName}

A value between 0 and 100 that specifies the sampling percentage to use when generating statistics for this table. This value defaults to 10 percent, and is only used if `estimateStatistics` is set to `true`.
{: .paramDefnFirst}
</div>


## bulkImportHFile {#bulkimporthfile}

This method efficiently imports data into your Splice Machine database by first generating HFiles and then importing those HFiles; it is the same as using the Splice Machine [`SYSCS_UTIL.BULK_IMPORT_HFILE`](sqlref_sysprocs_importhfile.html) system procedure.

You can either pass the data to this method in a DataFrame, or you can pass the data in an RDD, and pass in a structure (the Catalyst schema) that specifies the organization of the data.

<div class="fcnWrapperWide" markdown="1"><pre>
bulkImportHFile( dataFrame: DataFrame,
                 schemaTableName: String,
                 options: scala.collection.mutable.Map[String, String] ): Unit

bulkImportHFile( rdd: JavaRDD[Row],
                 schema: StructType,
                 schemaTableName: String,
                 options: scala.collection.mutable.Map[String, String] ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
dataFrame
{: .paramName}

The DataFrame containing the rows that you want imported into your database table.
{: .paramDefnFirst}

schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

rdd
{: .paramName}

The RDD containing the data the you want imported into your database table.
{: .paramDefnFirst}

The Catalyst schema of the master table.
{: .paramName}

A structure that specifies the layout of the data in the RDD.
{: .paramDefnFirst}

options
{: .paramName}

A collection of (key, value) pairs specifying the import options. For example, you can specify that sampling is not to be used with a statement like this:
````
    val bulkImportOptions = scala.collection.mutable.Map( "skipSampling" -> "true" )
````

{: .paramDefnFirst}
</div>

## createTable  {#createtable}

This method creates a new table in your Splice Machine database; it is the same as using the Splice Machine [`CREATE TABLE`](sqlref_statements_createtable.html) SQL statement.

<div class="fcnWrapperWide" markdown="1"><pre>
createTable( tableName: String,
             structType: StructType,
             keys: Seq[String],
             createTableOptions: String ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
tableName
{: .paramName}

The name of the table.
{: .paramDefnFirst}

structType
{: .paramName}

A structure that specifies the table's schema.
{: .paramDefnFirst}

keys
{: .paramName}

A sequence (comma-separated list) of keys for the table.
{: .paramDefnFirst}

createTableOptions
{: .paramName}

A string that specifies the table options.
{: .paramDefnFirst}
</div>


## delete {#delete}

This method deletes the contents of a Spark DataFrame or Spark RDD from a Splice Machine table; it is the same as using the Splice Machine [`DELETE FROM`](sqlref_statements_delete.html) SQL statement.

You can either pass the data to this method in a DataFrame, or you can pass the data in an RDD, and pass in a structure that specifies the organization of the data.

<div class="fcnWrapperWide" markdown="1"><pre>
delete( dataFrame: DataFrame,
        schemaTableName: String ): Unit

delete( rdd: JavaRDD[Row],
        schema: StructType,
        schemaTableName: String ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
dataFrame
{: .paramName}

The DataFrame containing the rows that you want deleted from your database table.
{: .paramDefnFirst}

schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

rdd
{: .paramName}

The RDD containing the data the you want deleted from your database table.
{: .paramDefnFirst}

The Catalyst schema of the master table.
{: .paramName}

A structure that specifies the layout of the data in the RDD.
{: .paramDefnFirst}
</div>


## df and internalDf {#df}

These methods executes an SQL string within Splice Machine and returns the results in a Spark DataFrame.

The only difference between `df` and `internalDf` methods is that the `internalDf` method runs internally and temporarily persists data on HDFS; this has a slight performance impact, but allows for checking permissions on Views. For more information, please see the [Accessing Database Objects](developers_spark_adapter.html#access) section in our *Using the Native Spark DataSource* topic.
{: .noteIcon}

<div class="fcnWrapperWide" markdown="1"><pre>
df( sql: String ): Dataset[Row]

internalDf( sql: String ): Dataset[Row]</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
sql
{: .paramName}

The SQL string.
{: .paramDefnFirst}
</div>


## dropTable  {#droptable}

This method removes the specified table; it is the same as using the Splice Machine [`DROP TABLE`](sqlref_statements_droptable.html) SQL statement.

You can pass the schema and table names into this method separately, or in combined (`schema.table`) format.

<div class="fcnWrapperWide" markdown="1"><pre>
dropTable(schemaTableName: String): Unit

dropTable( schemaName: String,
           tableName: String ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

schemaName
{: .paramName}

The schema name.
{: .paramDefnFirst}

tableName
{: .paramName}

The name of the table.
{: .paramDefnFirst}
</div>

## export  {#export}
This method exports a dataFrame in CSV format.

<div class="fcnWrapperWide" markdown="1"><pre>
export( dataFrame: DataFrame,
        location: String,
        compression: Boolean,
        replicationCount: Int,
        fileEncoding: String,
        fieldSeparator: String,
        quoteCharacter: String): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
dataFrame
{: .paramName}
The dataFrame that you want to export.
{: .paramDefnFirst}

location
{: .paramName}
The directory in which you want the export file(s) written.
{: .paramDefnFirst}

compression
{: .paramName}
Whether or not to compress the exported files. You can specify one of
the following values:
{: .paramDefnFirst}

<div markdown="0">
    <table summary="Possible values for compression">
            <col />
            <col />
            <thead>
                <tr>
                    <th>Value</th>
                    <th>Description</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><code>true</code></td>
                    <td>The exported files are compressed using <code>deflate/gzip</code>.</td>
                </tr>
                <tr>
                    <td><code>false</code></td>
                    <td>Exported files are not compressed.</td>
                </tr>
            </tbody>
        </table>
</div>

replicationCount
{: .paramName}

The file system block replication count to use for the exported
CSV files.
{: .paramDefnFirst}

You can specify any positive integer value. The default value is `1`.
{: .paramDefn}

fileEncoding
{: .paramName}
The character set encoding to use for the exported CSV files.
{: .paramDefnFirst}

You can specify any character set encoding that is supported by the
Java Virtual Machine (JVM). The default encoding is `UTF-8`.
{: .paramDefn}
fieldSeparator
{: .paramName}

The character to use for separating fields in the exported CSV files.
{: .paramDefnFirst}
The default separator character is the comma (`,`).
{: .paramDefn}

quoteCharacter
{: .paramName}
The character to use for quoting output in the exported CSV files.
{: .paramDefnFirst}
The default quote character is the double quotation mark (`"`).
{: .paramDefn}
</div>

## exportBinary  {#exportbinary}
This method exports a dataFrame in binary format, generating one or
more binary files, which are stored in the directory that you specify in the
`location` parameter. More than one output file can be generated to
enhance the parallelism and performance of this operation.

<div class="fcnWrapperWide" markdown="1"><pre>
exportBinary( dataFrame: DataFrame,
              location: String,
              compression: Boolean,
              format: String): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
dataFrame
{: .paramName}
The dataFrame that you want to export.
{: .paramDefnFirst}

location
{: .paramName}
The directory in which you want the export file(s) written.
{: .paramDefnFirst}

compression
{: .paramName}
Whether or not to compress the exported files. You can specify one of
the following values:
{: .paramDefnFirst}

<div markdown="0">
    <table summary="Possible values for compression">
            <col />
            <col />
            <thead>
                <tr>
                    <th>Value</th>
                    <th>Description</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><code>true</code></td>
                    <td>The exported files are compressed using <code>Snappy</code>.</td>
                </tr>
                <tr>
                    <td><code>false</code></td>
                    <td>Exported files are not compressed.</td>
                </tr>
            </tbody>
        </table>
</div>
If `compression=true`, then each of the generated files is named with
this format: `part-r-<N>.snappy.parquet`; if not, then each file is named with this format: `part-r-<N>.parquet`. In either case, the value of `N` is a sequence of numbers and letters.
{: .paramDefn}

format
{: .paramName}
The format in which to write the exported file(s). The only format supported at this time is `parquet`.
{: .paramDefnFirst}
</div>

## getConnection {#getconnection}
This method returns the current connection.

<div class="fcnWrapperWide" markdown="1"><pre>
getConnection(): Connection</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

## getSchema {#getschema}
This method returns the Catalyst schema of the specified table.

<div class="fcnWrapperWide" markdown="1"><pre>
getSchema( schemaTableName: String ): StructType</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}
</div>

## insert {#insert}

This method inserts the contents of a Spark DataFrame or Spark RDD into a Splice Machine table; it is the same as using the Splice Machine [`INSERT INTO`](sqlref_statements_insert.html) SQL statement.

You can either pass the data to this method in a DataFrame, or you can pass the data in an RDD, and pass in a structure that specifies the organization of the data.

<div class="fcnWrapperWide" markdown="1"><pre>
insert( dataFrame: DataFrame,
        schemaTableName: String ): Unit

insert( rdd: JavaRDD[Row],
        schema: StructType,
        schemaTableName: String ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
dataFrame
{: .paramName}

The DataFrame containing the rows that you want inserted into your database table.
{: .paramDefnFirst}

schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

rdd
{: .paramName}

The RDD containing the data the you want inserted into your database table.
{: .paramDefnFirst}

schema
{: .paramName}

The Catalyst schema of the master table.
{: .paramDefnFirst}
</div>

## rdd and internalRdd {#rdd}

These methods creates a Spark RDD from a Splice Machine table.

The only difference between the `rdd` and `internalRdd` methods is that the `internalRdd` method runs internally and temporarily persists data on HDFS; this has a slight performance impact, but allows for checking permissions on Views. For more information, please see the [Accessing Database Objects](developers_spark_adapter.html#access) section in our *Using the Native Spark DataSource* topic.
{: .noteIcon}

<div class="fcnWrapperWide" markdown="1"><pre>
rdd( schemaTableName: String,
     columnProjection: Seq[String] = Nil ): RDD[Row]

internalRdd( schemaTableName: String,
     columnProjection: Seq[String] = Nil ): RDD[Row]</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

columnProjection
{: .paramName}

The names of the columns in the underlying table that you want to project into the RDD; this is a comma-separated list of strings.
{: .paramDefnFirst}
</div>

## splitAndInsert {#splitandinsert}

This method improves the performance of inserting data from the Native DataSource: instead of inserting into a single HBase region and having HBase split that region, we pre-split the table based on the data we're inserting, and then insert the dataFrame. The table splits are computed by sampling the data in the dataFrame; Splice Machine uses the sampling percentage specified by the `sampleFraction` parameter value.

<div class="fcnWrapperWide" markdown="1"><pre>
splitAndInsert( dataFrame: DataFrame,
                schemaTableName: String,
                sampleFraction: Double): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
dataFrame
{: .paramName}
The dataFrame to sample.
{: .paramDefnFirst}

schemaTableName
{: .paramName}
The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

sampleFraction
{: .paramName}
A value between 0 and 1 that specifies the percentage of data in the dataFrame that should be sampled to determine the splits. For example, specify `0.005` if you want 0.5% of the data sampled.
{: .paramDefnFirst}
</div>

## tableExists {#tableexists}

This method returns `true` if the specified table exists in your Splice Machine database.

You can pass the schema and table names into this method separately, or in combined (`schema.table`) format.

<div class="fcnWrapperWide" markdown="1"><pre>
tableExists( schemaTableName: String ): Boolean

tableExists( schemaName: String,
             tableName: String ): Boolean</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

schemaName
{: .paramName}

The schema name.
{: .paramDefnFirst}

tableName
{: .paramName}

The name of the table.
{: .paramDefnFirst}
</div>

## truncateTable {#truncatetable}

This method quickly removes all content from the specified table and returns it to its initial empty state.

It is the same as using the Splice Machine [`TRUNCATE TABLE`](sqlref_statements_truncatetable.html) SQL statement.

<div class="fcnWrapperWide" markdown="1"><pre>
truncateTable( tableName: String ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
tableName
{: .paramName}

The name of the table.
{: .paramDefnFirst}
</div>

## update {#update}

This method updates a Splice Machine table using the contents of a Spark DataFrame or Spark RDD from a Splice Machine table; it is the same as using the Splice Machine [`DELETE FROM`](sqlref_statements_delete.html) SQL statement.

You can either pass the data to this method in a DataFrame, or you can pass the data in an RDD, and pass in a structure that specifies the organization of the data.

<div class="fcnWrapperWide" markdown="1"><pre>
update( dataFrame: DataFrame,
        schemaTableName: String ): Unit

update( rdd: JavaRDD[Row],
        schema: StructType,
        schemaTableName: String ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
dataFrame
{: .paramName}

The DataFrame containing the rows that you want updated in your database table.
{: .paramDefnFirst}

schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

rdd
{: .paramName}

The RDD containing the data the you want updated in your database table.
{: .paramDefnFirst}

schema
{: .paramName}

The Catalyst schema of the master table.
{: .paramDefnFirst}
</div>

## upsert {#upsert}

This method upserts (inserts new records and updates existing records) the contents of a Spark DataFrame or Spark RDD into a Splice Machine table; it is the same as using the Splice Machine [`SYSCS_UTIL.UPSERT_DATA_FROM_FILE`](sqlref_sysprocs_upsertdata.html) system procedure.

You can either pass the data to this method in a DataFrame, or you can pass the data in an RDD, and pass in a structure that specifies the organization of the data.

<div class="fcnWrapperWide" markdown="1"><pre>
upsert(dataFrame: DataFrame,
       schemaTableName: String): Unit

upsert(rdd: JavaRDD[Row],
       schema: StructType,
       schemaTableName: String): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
dataFrame
{: .paramName}

The DataFrame containing the rows that you want inserted into your database table.
{: .paramDefnFirst}

schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

rdd
{: .paramName}

The RDD containing the data the you want inserted into your database table.
{: .paramDefnFirst}

schema
{: .paramName}

The Catalyst schema of the master table.
{: .paramDefnFirst}
</div>

## See Also
* [Using the Splice Machine Native Spark DataSource](developers_spark_adapter.html)
* [Using Spark Submit](developers_spark_submit.html)
* [Using our Native Spark DataSource with Zeppelin](developers_spark_zeppelin.html)

</div>
</section>
