---
title: Splice Machine Spark Adapter Methods
summary: Methods Available in the Splice Machine Spark Adapter.
keywords: spark, adapter, splicemachineContext
toc: false
compatible_version: 2.7
product: all
sidebar: developers_sidebar
permalink: developers_spark_methods.html
folder: DeveloperTopics/Spark
---
<section>
<div class="TopicContent" data-swiftype-index="true" markdown="1">

# The Spark Adapter Methods {#methods}

This topic describes the following methods of the `SplicemachineContext` class:

* [`analyzeSchema`](#analyzeschema)
* [`analyzeTable`](#analyzetable)
* [`bulkImportHFile`](#bulkimporthfile)
* [`createTable`](#createtable)
* [`delete`](#delete)
* [`df` and `internalDf`](#df)
* [`dropTable`](#droptable)
* [`insert`](#insert)
* [`rdd` and `internalRdd`](#rdd)
* [`tableExists`](#tableexists)
* [`truncateTable`](#truncatetable)
* [`update`](#update)
* [`upsert`](#upsert)

## analyzeSchema {#analyzeSchema}

This method collects statistics for an entire schema; it is the same as using the [`ANALYZE SCHEMA`](cmdlineref_analyze.html) `splice>` command line.

<div class="fcnWrapperWide" markdown="1"><pre>
analyzeSchema(schemaName: String): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
schemaName
{: .paramName}

The name of the schema that you want analyzed.
{: .paramDefnFirst}
</div>


## analyzeTable

This method collects statistics for a specific table; it is the same as using the [`ANALYZE TABLE`](cmdlineref_analyze.html) `splice>` command line.

<div class="fcnWrapperWide" markdown="1"><pre>
analyzeTable( tableName: String,
              estimateStatistics: Boolean = false,
              samplePercent: Double = 0.10 ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
tableName
{: .paramName}

The name of the table that you want analyzed.
{: .paramDefnFirst}

estimateStatistics
{: .paramName}

A Boolean that specifies whether you want statistics generated by sampling the specified sampling percentage of the table. This can significantly reduce the overhead associated with generating statistics. Setting this parameter to `false` specifies that statistics are to be generated based on the full table.
{: .paramDefnFirst}

samplePercent
{: .paramName}

A value between 0 and 100 that specifies the sampling percentage to use when generating statistics for this table. This value defaults to 10 percent, and is only used if `estimateStatistics` is set to `true`.
{: .paramDefnFirst}
</div>


## bulkImportHFile {#bulkimporthfile}

This method efficiently imports data into your Splice Machine database by first generating HFiles and then importing those HFiles; it is the same as using the Splice Machine [`SYSCS_UTIL.BULK_IMPORT_HFILE`](sqlref_sysprocs_importhfile.html) system procedure.

You can either pass the data to this method in a DataFrame, or you can pass the data in an RDD, and pass in a structure that specifies the organization of the data.

<div class="fcnWrapperWide" markdown="1"><pre>
bulkImportHFile( dataFrame: DataFrame,
                 schemaTableName: String,
                 options: scala.collection.mutable.Map[String, String] ): Unit

bulkImportHFile( rdd: JavaRDD[Row],
                 schema: StructType,
                 schemaTableName: String,
                 options: scala.collection.mutable.Map[String, String] ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
dataFrame
{: .paramName}

The DataFrame containing the rows that you want imported into your database table.
{: .paramDefnFirst}

schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

rdd
{: .paramName}

The RDD containing the data the you want imported into your database table.
{: .paramDefnFirst}

schema
{: .paramName}

A structure that specifies the layout of the data in the RDD.
{: .paramDefnFirst}

options
{: .paramName}

A collection of (key, value) pairs specifying the import options. For example, you can specify that sampling is not to be used with a statement like this:
````
    val bulkImportOptions = scala.collection.mutable.Map( "skipSampling" -> "true" )
````

{: .paramDefnFirst}
</div>

## createTable  {#createtable}

This method creates a new table in your Splice Machine database; it is the same as using the Splice Machine [`CREATE TABLE`](sqlref_statements_createtable.html) SQL statement.

<div class="fcnWrapperWide" markdown="1"><pre>
createTable( tableName: String,
             structType: StructType,
             keys: Seq[String],
             createTableOptions: String ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
tableName
{: .paramName}

The name of the table.
{: .paramDefnFirst}

structType
{: .paramName}

A structure that specifies the table's schema.
{: .paramDefnFirst}

keys
{: .paramName}

A sequence (comma-separated list) of keys for the table.
{: .paramDefnFirst}

createTableOptions
{: .paramName}

A string that specifies the table options.
{: .paramDefnFirst}
</div>


## delete {#delete}

This method deletes the contents of a Spark DataFrame or Spark RDD from a Splice Machine table; it is the same as using the Splice Machine [`DELETE FROM`](sqlref_statements_delete.html) SQL statement.

You can either pass the data to this method in a DataFrame, or you can pass the data in an RDD, and pass in a structure that specifies the organization of the data.

<div class="fcnWrapperWide" markdown="1"><pre>
delete( dataFrame: DataFrame,
        schemaTableName: String ): Unit

delete( rdd: JavaRDD[Row],
        schema: StructType,
        schemaTableName: String ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
dataFrame
{: .paramName}

The DataFrame containing the rows that you want deleted from your database table.
{: .paramDefnFirst}

schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

rdd
{: .paramName}

The RDD containing the data the you want deleted from your database table.
{: .paramDefnFirst}

schema
{: .paramName}

A structure that specifies the layout of the data in the RDD.
{: .paramDefnFirst}
</div>


## df and internalDf {#df}

These methods executes an SQL string within Splice Machine and returns the results in a Spark DataFrame.

The only difference between `df` and `internalDf` methods is that the `internalDf` method runs internally and temporarily persists data on HDFS; this has a slight performance impact, but allows for checking permissions on Views.
{: .noteIcon}

<div class="fcnWrapperWide" markdown="1"><pre>
df( sql: String ): Dataset[Row]

internalDf( sql: String ): Dataset[Row]</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
sql
{: .paramName}

The SQL string.
{: .paramDefnFirst}
</div>


## dropTable  {#droptable}

This method removes the specified table; it is the same as using the Splice Machine [`DROP TABLE`](sqlref_statements_droptable.html) SQL statement.

You can pass the schema and table names into this method separately, or in combined (`schema.table`) format.

<div class="fcnWrapperWide" markdown="1"><pre>
dropTable(schemaTableName: String): Unit

dropTable( schemaName: String,
           tableName: String ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

schemaName
{: .paramName}

The schema name.
{: .paramDefnFirst}

tableName
{: .paramName}

The name of the table.
{: .paramDefnFirst}
</div>

## insert {#insert}

This method inserts the contents of a Spark DataFrame or Spark RDD into a Splice Machine table; it is the same as using the Splice Machine [`INSERT INTO`](sqlref_statements_insert.html) SQL statement.

You can either pass the data to this method in a DataFrame, or you can pass the data in an RDD, and pass in a structure that specifies the organization of the data.

<div class="fcnWrapperWide" markdown="1"><pre>
insert( dataFrame: DataFrame,
        schemaTableName: String ): Unit

insert( rdd: JavaRDD[Row],
        schema: StructType,
        schemaTableName: String ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
dataFrame
{: .paramName}

The DataFrame containing the rows that you want inserted into your database table.
{: .paramDefnFirst}

schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

rdd
{: .paramName}

The RDD containing the data the you want inserted into your database table.
{: .paramDefnFirst}

schema
{: .paramName}

A structure that specifies the layout of the data in the RDD.
{: .paramDefnFirst}
</div>

## rdd and internalRdd {#rdd}

These methods creates a Spark RDD from a Splice Machine table.

The only difference between the `rdd` and `internalRdd` methods is that the `internalRdd` method runs internally and temporarily persists data on HDFS; this has a slight performance impact, but allows for checking permissions on Views.
{: .noteIcon}

<div class="fcnWrapperWide" markdown="1"><pre>
rdd( schemaTableName: String,
     columnProjection: Seq[String] = Nil ): RDD[Row]

internalRdd( schemaTableName: String,
     columnProjection: Seq[String] = Nil ): RDD[Row]</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

columnProjection
{: .paramName}

The names of the columns in the underlying table that you want to project into the RDD; this is a comma-separated list of strings.
{: .paramDefnFirst}
</div>

## tableExists {#tableexists}

This method returns `true` if the specified table exists in your Splice Machine database.

You can pass the schema and table names into this method separately, or in combined (`schema.table`) format.

<div class="fcnWrapperWide" markdown="1"><pre>
tableExists( schemaTableName: String ): Boolean

tableExists( schemaName: String,
             tableName: String ): Boolean</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

schemaName
{: .paramName}

The schema name.
{: .paramDefnFirst}

tableName
{: .paramName}

The name of the table.
{: .paramDefnFirst}
</div>

## truncateTable {#truncatetable}

This method quickly removes all content from the specified table and returns it to its initial empty state.

It is the same as using the Splice Machine [`TRUNCATE TABLE`](sqlref_statements_truncatetable.html) SQL statement.

<div class="fcnWrapperWide" markdown="1"><pre>
truncateTable( tableName: String ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
tableName
{: .paramName}

The name of the table.
{: .paramDefnFirst}
</div>

## update {#update}

This method updates a Splice Machine table using the contents of a Spark DataFrame or Spark RDD from a Splice Machine table; it is the same as using the Splice Machine [`DELETE FROM`](sqlref_statements_delete.html) SQL statement.

You can either pass the data to this method in a DataFrame, or you can pass the data in an RDD, and pass in a structure that specifies the organization of the data.

<div class="fcnWrapperWide" markdown="1"><pre>
update( dataFrame: DataFrame,
        schemaTableName: String ): Unit

update( rdd: JavaRDD[Row],
        schema: StructType,
        schemaTableName: String ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
dataFrame
{: .paramName}

The DataFrame containing the rows that you want updated in your database table.
{: .paramDefnFirst}

schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

rdd
{: .paramName}

The RDD containing the data the you want updated in your database table.
{: .paramDefnFirst}

schema
{: .paramName}

A structure that specifies the layout of the data in the RDD.
{: .paramDefnFirst}
</div>

## upsert {#upsert}

This method upserts (inserts new records and updates existing records) the contents of a Spark DataFrame or Spark RDD into a Splice Machine table; it is the same as using the Splice Machine [`SYSCS_UTIL.UPSERT_DATA_FROM_FILE`](sqlref_sysprocs_upsertdata.html) system procedure.

You can either pass the data to this method in a DataFrame, or you can pass the data in an RDD, and pass in a structure that specifies the organization of the data.

<div class="fcnWrapperWide" markdown="1"><pre>
upsert(dataFrame: DataFrame,
       schemaTableName: String): Unit

upsert(rdd: JavaRDD[Row],
       schema: StructType,
       schemaTableName: String): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
dataFrame
{: .paramName}

The DataFrame containing the rows that you want inserted into your database table.
{: .paramDefnFirst}

schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

rdd
{: .paramName}

The RDD containing the data the you want inserted into your database table.
{: .paramDefnFirst}

schema
{: .paramName}

A structure that specifies the layout of the data in the RDD.
{: .paramDefnFirst}
</div>

## See Also
* [Using the Splice Machine Spark Adapter](developers_spark_adapter.html)
* [Using Spark Submit](developers_spark_submit.html)
* [Using our Spark Adapter with Zeppelin](developers_spark_zeppelin.html)

</div>
</section>
