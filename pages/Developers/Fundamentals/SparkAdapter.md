---
title: Using the Splice Machine Spark Adapter
summary: Overview and examples of using the Splice Machine Spark Adapter.
keywords: spark, adapter, splicemachineContext
toc: false
product: all
sidebar: developers_sidebar
permalink: developers_fundamentals_sparkadapter.html
folder: Developers/Fundamentals
---
<section>
<div class="TopicContent" data-swiftype-index="true" markdown="1">
# Using the Splice Machine Spark Adapter

This topic describes the API for the Splice Machine Spark Adapter and provides examples of using the API in scala programming and in Apache Zeppelin notebooks. This topic is organized into these sections:

* [About the Splice Machine Spark Adapter](#about)
* [Prerequisites for Using the Adapter](#prereq)
* [The Spark Adapter Methods](#methods)
* [Examples using Scala](#sparksubmit)
* [Examples of Using the Spark Adapter in Zeppelin Notebooks](#zepexamples)

## About the Splice Machine Spark Adapter {#about}

The Splice Machine Spark Adapter allows you to directly connect Spark DataFrames and Splice Machine database tables. You can efficiently insert, upsert, select, update, and delete data in your Splice Machine tables directly from Spark in a transactionally consistent manner. To use the adapter, you simmply instantiate a `SplicemachineContext` object in your Spark code.

## Prerequisites for Using the Adapter {#prereq}

To use the adapter, you must:

1. Make sure that `splice` space has read, write, and create permissions on HBase. For example:
   <div class="preWrapperWide" markdown="1">
       hbase(main):003:0> grant 'someuser', 'RWC', '@splice'
   {: .ShellCommand}
   </div>

2. Make sure that each user who is going to use the Splice Machine Spark Adapter has `execute` permission on the `SYSCS_UTIL.SYSCS_HDFS_OPERATION` system procedure.

   `SYSCS_UTIL.SYSCS_HDFS_OPERATION` is a Splice Machine system procedure that is used internally to efficiently perform direct HDFS operations. This procedure *is not documented* because it is intended only for use by the Splice Machine code itself; however, the Spark Adapter uses it, so any user of the Adapter must have permission to execute the `SYSCS_UTIL.SYSCS_HDFS_OPERATION` procedure.
   {: .noteIcon}

   Here's an example of granting `execute` permission for two users:
   <div class="preWrapperWide" markdown="1">
       splice> grant execute on procedure SYSCS_UTIL.SYSCS_HDFS_OPERATION to someuser;
       0 rows inserted/updated/deleted
       splice> grant execute on procedure SYSCS_UTIL.SYSCS_HDFS_OPERATION to anotheruser;
       0 rows inserted/updated/deleted
   {: .Example}
   </div>

## The Spark Adapter Methods {#methods}

This section describes the following methods of the `SplicemachineContext` class:

* [`analyzeSchema`](#analyzeschema)
* [`analyzeTable`](#analyzetable)
* [`bulkImportHFile`](#bulkimporthfile)
* [`createTable`](#createtable)
* [`delete`](#delete)
* [`df`](#df)
* [`dropTable`](#droptable)
* [`insert`](#insert)
* [`rdd`](#rdd)
* [`tableExists`](#tableexists)
* [`truncateTable`](#truncatetable)
* [`update`](#update)
* [`upsert`](#upsert)

### analyzeSchema {#analyzeSchema}

This method collects statistics for an entire schema; it is the same as using the [`ANALYZE SCHEMA`](cmdlineref_analyze.html`) `splice>` command line.

<div class="fcnWrapperWide" markdown="1"><pre>
analyzeSchema(schemaName: String): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
schemaName
{: .paramName}

The name of the schema that you want analyzed.
{: .paramDefnFirst}
</div>


### analyzeTable

This method collects statistics for a specific table; it is the same as using the [`ANALYZE TABLE`](cmdlineref_analyze.html`) `splice>` command line.

<div class="fcnWrapperWide" markdown="1"><pre>
analyzeTable( tableName: String,
              estimateStatistics: Boolean = false,
              samplePercent: Double = 0.10 ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
tableName
{: .paramName}

The name of the table that you want analyzed.
{: .paramDefnFirst}

estimateStatistics
{: .paramName}

A Boolean that specifies whether you want statistics generated by sampling the specified sampling percentage of the table. This can significantly reduce the overhead associated with generating statistics. Setting this parameter to `false` specifies that statistics are to be generated based on the full table.
{: .paramDefnFirst}

samplePercent
{: .paramName}

A value between 0 and 100 that specifies the sampling percentage to use when generating statistics for this table. This value defaults to 10 percent, and is only used if `estimateStatistics` is set to `true`.
{: .paramDefnFirst}
</div>


### bulkImportHFile {#bulkimporthfile}

This method efficiently imports data into your Splice Machine database by first generating HFiles and then importing those HFiles; it is the same as using the Splice Machine [`SYSCS_UTIL.BULK_IMPORT_HFILE`](sqlref_sysprocs_importhfile.html) system procedure.

You can either pass the data to this method in a DataFrame, or you can pass the data in an RDD, and pass in a structure that specifies the organization of the data.

<div class="fcnWrapperWide" markdown="1"><pre>
bulkImportHFile( dataFrame: DataFrame,
                 schemaTableName: String,
                 options: scala.collection.mutable.Map[String, String] ): Unit

bulkImportHFile( rdd: JavaRDD[Row],
                 schema: StructType,
                 schemaTableName: String,
                 options: scala.collection.mutable.Map[String, String] ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
dataFrame
{: .paramName}

The DataFrame containing the rows that you want imported into your database table.
{: .paramDefnFirst}

schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

rdd
{: .paramName}

The RDD containing the data the you want imported into your database table.
{: .paramDefnFirst}

schema
{: .paramName}

A structure that specifies the layout of the data in the RDD.
{: .paramDefnFirst}

options
{: .paramName}

A collection of (key, value) pairs specifying the import options. For example, you can specify that sampling is not to be used with a statement like this:
````
    val bulkImportOptions = scala.collection.mutable.Map( "skipSampling" -> "true" )
````

{: .paramDefnFirst}
</div>

### createTable  {#createtable}

This method creates a new table in your Splice Machine database; it is the same as using the Splice Machine [`CREATE TABLE`](sqlref_statements_createtable.html) SQL statement.

<div class="fcnWrapperWide" markdown="1"><pre>
createTable( tableName: String,
             structType: StructType,
             keys: Seq[String],
             createTableOptions: String ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
tableName
{: .paramName}

The name of the table.
{: .paramDefnFirst}

structType
{: .paramName}

A structure that specifies the table's schema.
{: .paramDefnFirst}

keys
{: .paramName}

A sequence (comma-separated list) of keys for the table.
{: .paramDefnFirst}

createTableOptions
{: .paramName}

A string that specifies the table options.
{: .paramDefnFirst}
</div>


### delete {#delete}

This method deletes the contents of a Spark DataFrame or Spark RDD from a Splice Machine table; it is the same as using the Splice Machine [`DELETE FROM`](sqlref_statements_delete.html) SQL statement.

You can either pass the data to this method in a DataFrame, or you can pass the data in an RDD, and pass in a structure that specifies the organization of the data.

<div class="fcnWrapperWide" markdown="1"><pre>
delete( dataFrame: DataFrame,
        schemaTableName: String ): Unit

delete( rdd: JavaRDD[Row],
        schema: StructType,
        schemaTableName: String ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
dataFrame
{: .paramName}

The DataFrame containing the rows that you want deleted from your database table.
{: .paramDefnFirst}

schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

rdd
{: .paramName}

The RDD containing the data the you want deleted from your database table.
{: .paramDefnFirst}

schema
{: .paramName}

A structure that specifies the layout of the data in the RDD.
{: .paramDefnFirst}
</div>


### df {#df}

This method executes an SQL string within Splice Machine and returns the results in a Spark DataFrame.

<div class="fcnWrapperWide" markdown="1"><pre>
df( sql: String ): Dataset[Row]</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
sql
{: .paramName}

The SQL string.
{: .paramDefnFirst}
</div>


### dropTable  {#droptable}

This method removes the specified table; it is the same as using the Splice Machine [`DROP TABLE`](sqlref_statements_droptable.html) SQL statement.

You can pass the schema and table names into this method separately, or in combined (`schema.table`) format.

<div class="fcnWrapperWide" markdown="1"><pre>
dropTable(schemaTableName: String): Unit

dropTable( schemaName: String,
           tableName: String ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

schemaName
{: .paramName}

The schema name.
{: .paramDefnFirst}

tableName
{: .paramName}

The name of the table.
{: .paramDefnFirst}
</div>


### insert {#insert}

This method inserts the contents of a Spark DataFrame or Spark RDD into a Splice Machine table; it is the same as using the Splice Machine [`INSERT INTO`](sqlref_statements_insert.html) SQL statement.

You can either pass the data to this method in a DataFrame, or you can pass the data in an RDD, and pass in a structure that specifies the organization of the data.

<div class="fcnWrapperWide" markdown="1"><pre>
insert( dataFrame: DataFrame,
        schemaTableName: String ): Unit

insert( rdd: JavaRDD[Row],
        schema: StructType,
        schemaTableName: String ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
dataFrame
{: .paramName}

The DataFrame containing the rows that you want inserted into your database table.
{: .paramDefnFirst}

schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

rdd
{: .paramName}

The RDD containing the data the you want inserted into your database table.
{: .paramDefnFirst}

schema
{: .paramName}

A structure that specifies the layout of the data in the RDD.
{: .paramDefnFirst}
</div>


### rdd {#rdd}

This method creates a Spark RDD from a Splice Machine table.

<div class="fcnWrapperWide" markdown="1"><pre>
rdd( schemaTableName: String,
     columnProjection: Seq[String] = Nil ): RDD[Row]</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

columnProjection
{: .paramName}

The names of the columns in the underlying table that you want to project into the RDD; this is a comma-separated list of strings.
{: .paramDefnFirst}
</div>


### tableExists {#tableexists}

This method returns `true` if the specified table exists in your Splice Machine database.

You can pass the schema and table names into this method separately, or in combined (`schema.table`) format.

<div class="fcnWrapperWide" markdown="1"><pre>
tableExists( schemaTableName: String ): Boolean

tableExists( schemaName: String,
             tableName: String ): Boolean</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

schemaName
{: .paramName}

The schema name.
{: .paramDefnFirst}

tableName
{: .paramName}

The name of the table.
{: .paramDefnFirst}
</div>


### truncateTable {#truncatetable}

This method quickly removes all content from the specified table and returns it to its initial empty state.

It is the same as using the Splice Machine [`TRUNCATE TABLE`](sqlref_statements_truncatetable.html) SQL statement.

<div class="fcnWrapperWide" markdown="1"><pre>
truncateTable( tableName: String ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
tableName
{: .paramName}

The name of the table.
{: .paramDefnFirst}
</div>


### update {#update}

This method updates a Splice Machine table using the contents of a Spark DataFrame or Spark RDD from a Splice Machine table; it is the same as using the Splice Machine [`DELETE FROM`](sqlref_statements_delete.html) SQL statement.

You can either pass the data to this method in a DataFrame, or you can pass the data in an RDD, and pass in a structure that specifies the organization of the data.

<div class="fcnWrapperWide" markdown="1"><pre>
update( dataFrame: DataFrame,
        schemaTableName: String ): Unit

update( rdd: JavaRDD[Row],
        schema: StructType,
        schemaTableName: String ): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
dataFrame
{: .paramName}

The DataFrame containing the rows that you want updated in your database table.
{: .paramDefnFirst}

schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

rdd
{: .paramName}

The RDD containing the data the you want updated in your database table.
{: .paramDefnFirst}

schema
{: .paramName}

A structure that specifies the layout of the data in the RDD.
{: .paramDefnFirst}
</div>


### upsert {#upsert}

This method upserts (inserts new records and updates existing records) the contents of a Spark DataFrame or Spark RDD into a Splice Machine table; it is the same as using the Splice Machine [`SYSCS_UTIL.UPSERT_DATA_FROM_FILE`](sqlref_sysprocs_upsertdata.html) system procedure.

You can either pass the data to this method in a DataFrame, or you can pass the data in an RDD, and pass in a structure that specifies the organization of the data.

<div class="fcnWrapperWide" markdown="1"><pre>
upsert(dataFrame: DataFrame,
       schemaTableName: String): Unit

upsert(rdd: JavaRDD[Row],
       schema: StructType,
       schemaTableName: String): Unit</pre>
{: .FcnSyntax xml:space="preserve"}
</div>

<div class="paramList" markdown="1">
dataFrame
{: .paramName}

The DataFrame containing the rows that you want inserted into your database table.
{: .paramDefnFirst}

schemaTableName
{: .paramName}

The combined schema and table names, in the form: `mySchema.myTable`.
{: .paramDefnFirst}

rdd
{: .paramName}

The RDD containing the data the you want inserted into your database table.
{: .paramDefnFirst}

schema
{: .paramName}

A structure that specifies the layout of the data in the RDD.
{: .paramDefnFirst}
</div>


## Examples using Scala {#sparksubmit}

In this section, we'll use `scala` to create a simple Splice Machine database table, and then access and modify that table.

### Setting Up the Splice Machine Adapter
You instantiate and object of the SplicemachineContext class to work with the Splice Machine Adapter. Here's some typical code:
<div class="preWrapperWide" markdown="1"><pre>
    // Create a Spark and SQL context
val sc = new SparkContext(sparkConf)
val sqlContext = new SQLContext(sc)

    // Comma-separated list of Splice Machine masters with port numbers
val master1 = "ip-xx-yy-1-zzz.ec2.internal:7051"
val master2 = "ip-xx-yy-2-yyy.ec2.internal:7051"
val master3 = "ip-xx-yy-3-yyy.ec2.internal:7051"
val Splice MachineMasters = Seq(master1, master2, master3).mkString(",")

    // Create an instance of a SplicemachineContext
val SpliceContext = new SplicemachineContext(Splice MachineMasters)</pre>
{: .Example}
</div>

### Creating a Table in Your Splice Machine Database
Now we'll use the Adapter to create a new table in our database, in 5 steps.

#### 1. Remove pre-existing table if necessary:
First, since we run this code frequently, we'll remove any  pre-existing version of our table from our database:
<div class="preWrapperWide" markdown="1"><pre>
    // Specify a table name
var Splice MachineTableName = "spark_Splice Machine_tbl"
if (SpliceContext.tableExists(Splice MachineTableName)) {
    SpliceContext.dropTable(Splice MachineTableName) }
}</pre>
{: .Example}
</div>

#### 2. Define a schema
Next we'll define the schema for our new table:
<div class="preWrapperWide" markdown="1"><pre>
val Splice MachineTableSchema = StructType(
        //  col name   type    nullable?
   StructField("id", IntegerType , false) ::
   StructField("make" , StringType, true ) ::
   StructField("model", StringType , true ) :: Nil)</pre>
{: .Example}
</div>

#### 3. Define the Primary Key
Let's make the ID column our primary key:
<div class="preWrapperWide" markdown="1"><pre>
val Splice MachinePrimaryKey = Seq("id")</pre>
{: .Example}
</div>

#### 4. Specify any Additional Options
We can specify any added options for our new table:
<div class="preWrapperWide" markdown="1"><pre>
val Splice MachineTableOptions = new CreateTableOptions()
Splice MachineTableOptions.setRangePartitionColumns(List("name").asJava).setNumReplicas(3)</pre>
{: .Example}
</div>

#### 5. Create the Table
And now we can use the `createTable` method to create our Splice Machine table:
<div class="preWrapperWide" markdown="1"><pre>
SpliceContext.createTable(
    Splice MachineTableName, Splice MachineTableSchema, Splice MachinePrimaryKey, Splice MachineTableOptions)</pre>
{: .Example}
</div>

### Inserting Data into the Table
Now we'll use the following 4 steps to insert data into our database.

#### 1. Import required functionality
<div class="preMarkerWide" markdown="1"><pre>
import sqlContext.implicits._</pre>
{: .Example}
</div>

#### 2. Create an RDD containing table data

<div class="preMarkerWide" markdown="1"><pre>
    // Define our case class *outside* our main method
case class Car(id:Int, make:String, model:String)

    // Define a list of cars based  on the Car class
val cars = Array(
   Car(1, "Toyota", "Camry"),
   Car(2, "Honda", "Accord"),
   Car(3, "Subaru", "Impreza"),
   Car(4, "Chevy", "Volt") )

    // Transform our list into an RDD
val carsRDD = sc.parallelize(cars)</pre>
{: .Example}
</div>


#### 3. Convert the RDD into a DataFrame
<div class="preMarkerWide" markdown="1"><pre>
val carsDF = carsRDD.toDF()

    // Define Splice Machine options used by various operations
val Splice MachineOptions: Map[String, String] = Map(
    "Splice Machine.table"  -> Splice MachineTableName,
    "Splice Machine.master" -> Splice MachineMasters )</pre>
{: .Example}
</div>

#### 4. Insert Data into Splice Machine table
Now we'll insert the contents of the DataFrame into our table:
<div class="preMarkerWide" markdown="1"><pre>
SpliceContext.insertRows(customersDF, Splice MachineTableName)</pre>
{: .Example}
</div>

### Selecting Data from Our Table
You can select data from a database table:
<div class="preMarkerWide" markdown="1"><pre>
sqlContext.read.options(Splice MachineOptions).Splice Machine.show
+----+---------+-------------+
|  id|make     |model        |
+----+---------+-------------+
|   1|Toyota   |Camry        |
|   2|Honda    |Accord       |
|   3|Subaru   |Impreza      |
|   4|Chevy    |Volt         |
+----+---------+-------------+</pre>
{: .Example}
</div>

### Updating Data in the Table
You can update existing data in a table:
<div class="preMarkerWide" markdown="1"><pre>
    // Create a DataFrame of updated rows
val modifiedCars    = Array(Car(4, "Chevy", "Bolt"))
val modifiedCarsRDD = sc.parallelize(modifiedCars)
val modifiedCarsDF  = modifiedCarsRDD.toDF()

    // Call update with our new and changed customers DataFrame
SpliceContext.updateRows(modifiedCarsDF, Splice MachineTableName)</pre>
{: .Example}
</div>

### Deleting Data
You can delete rows from a table:
<div class="preMarkerWide" markdown="1"><pre>
    // Let’s register our cars dataframe as a temporary table so we
    // refer to it in Spark SQL
carsDF.registerTempTable("cars")

    // Filter and create a keys-only DataFrame to be deleted from our table
val deleteKeysDF = sqlContext.sql("select car from cars where make='Subaru'")

    // Delete the rows from our Splice Machine table
SpliceContext.deleteRows(deleteKeysDF, Splice MachineTableName)</pre>
{: .Example}
</div>

### Dropping a Table
As you've already seen, it's easy to drop a table from your database:
<div class="preWrapperWide" markdown="1"><pre>
    // Specify a table name
var Splice MachineTableName = "spark_Splice Machine_tbl"
if (SpliceContext.tableExists(Splice MachineTableName)) {
    SpliceContext.dropTable(Splice MachineTableName) }</pre>
{: .Example}
</div>


## Examples of Using the Spark Adapter in Zeppelin Notebooks {#zepexamples}

This example in this section shows you how to use the Spark Adapter in an Apache Zeppelin notebook. We use the `%spark` and `%splicemachine` Zeppelin interpreters to create a simple Splice Machine database table, and then access and modify that table.

### Setting Up the Splice Machine Adapter
<div class="preMarkerWide" markdown="1"><pre>
%spark
import com.splicemachine.spark.splicemachine._
import com.splicemachine.derby.utils._

val JDBC_URL = "jdbc:splice://XXXX:1527/splicedb;user=splice;password=admin"
val SpliceContext = new SplicemachineContext(JDBC_URL)</pre>
{: .Example}
</div>

### Creating a Table in Your Splice Machine Database

#### 1. Remove pre-existing table if necessary:
<div class="preMarkerWide" markdown="1"><pre>
%splicemachine
create table carsTbl (number int primary key, make varchar(20), model varchar(20));</pre>
{: .Example}
</div>

#### 2. Create our Database Table
<div class="preMarkerWide" markdown="1"><pre>
%splicemachine
create table carsTbl (number int primary key, make varchar(20), model varchar(20));</pre>
{: .Example}
</div>

### Inserting Data into Our Table

First we'll create a Spark DataFrame and populate it with some data:
<div class="preMarkerWide" markdown="1"><pre>
%spark
val carsDF = Seq(
  (1, "Toyota", "Camry"),
  (2, "Honda", "Accord"),
  (3, "Subaru", "Impreza"),
  (4, "Chevy", "Volt")
).toDF("NUMBER", "MAKE", "MODEL")</pre>
{: .Example}
</div>

And then we'll insert that data into our Splice Machine table:
<div class="preMarkerWide" markdown="1"><pre>
%spark
SpliceContext.insert(carsDF, "SPLICE.CARSTBL")</pre>
{: .Example}
</div>

### Selecting Data
<div class="preMarkerWide" markdown="1"><pre>
%spark
SpliceContext.df("SELECT * FROM SPLICE.CARSTBL").show()</pre>
{: .Example}
</div>

### Updating Data
<div class="preMarkerWide" markdown="1"><pre>
%spark
val updateCarsDF = Seq(
    (1, "Toyota", "Rav 4 XLE"),
    (4, "Honda", "Accord Hybrid")
).toDF("NUMBER", "MAKE", "MODEL")
SpliceContext.update(updateCarsDF, "SPLICE.CARSTBL")</pre>
{: .Example}
</div>

### Deleting Data
<div class="preMarkerWide" markdown="1"><pre>
%spark
val deleteCarsDF = Seq(
    (1, "Toyota", "Rav 4 XLE"),
    (4, "Honda", "Accord Hybrid")
).toDF("NUMBER", "MAKE", "MODEL")
SpliceContext.delete(deleteCarsDF, "SPLICE.CARSTBL")</pre>
{: .Example}
</div>

### Dropping a Table
<div class="preMarkerWide" markdown="1"><pre>
%spark
if (SpliceContext.tableExists("SPLICE.CARSTBL")) {
    SpliceContext.dropTable("SPLICE.CARSTBL") }</pre>
{: .Example}
</div>


</div>
</section>
